# 条件生成模型

之前通过分类模型解决了句子分类的问题，本节中将实现基于prompt的条件生成模型。

## 1. 数据读取

首先读取webnlg数据集：

```python
from openprompt.data_utils.conditional_generation_dataset import WebNLGProcessor

dataset = {}
dataset['train'] = WebNLGProcessor().get_train_examples("datasets/CondGen/webnlg_2017/")
dataset['validation'] = WebNLGProcessor().get_dev_examples("datasets/CondGen/webnlg_2017/")
dataset['test'] = WebNLGProcessor().get_test_examples("datasets/CondGen/webnlg_2017/")
```

看一下具体的数据，`"text_a"`是输入数据，`”tgt_text”`是生成的答案：

```python
print(dataset['train'][666])
{
  "guid": "666",
  "label": null,
  "meta": {},
  "text_a": " | Texas : capital : Austin,_Texas",
  "text_b": "",
  "tgt_text": "Austin is the capital of Texas."
}
```

## 2. 读取PLM

依然使用T5模型：

```python
from openprompt.plms import load_plm

plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')
```

## 3. 定义Template

此处使用前缀模板类`PrefixTuningTemplate`构建模板：

```python
# Instantiating the PrefixTuning Template !
from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate
# we can use a plain text as the default setting
# i.e. 
# mytemplate = PrefixTuningTemplate(model=plm, tokenizer=tokenizer)
# is equal to 
# mytemplate = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text='{"placeholder":"text_a"} {"mask"}')
mytemplate = PrefixTuningTemplate(model=plm,  tokenizer=tokenizer, text=' {"placeholder":"text_a"} {"special": "<eos>"} {"mask"} ', using_decoder_past_key_values=False)

```

为了直观理解，展示一个包装过的效果：

```python
# To better understand how does the template wrap the example, we visualize one instance.
# You may observe that the example doesn't end with <|endoftext|> token. Don't worry, adding specific end-of-text token
# <eos> is a language-model-specific token. we will add it for you in the TokenizerWrapper once you pass `predict_eos_token=True`
wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) 
print(wrapped_example)

>>> {'text': '  | Aarhus_Airport : cityServed : "Aarhus, Denmark"', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<eos>', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': '0', 'tgt_text': 'The Aarhus is the airport of Aarhus, Denmark.'}]
```

## 4. 构建DataLoader

```python
from openprompt import PromptDataLoader
train_dataloader = PromptDataLoader(dataset=dataset["train"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256, 
    batch_size=5,shuffle=True, teacher_forcing=True, predict_eos_token=True, # be sure to pass predict_eos_token=True if your tempalte doesn't contain one, or you model may fail to stop generation.
    truncate_method="head")

validation_dataloader = PromptDataLoader(dataset=dataset["validation"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256, 
    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=True,
    truncate_method="head")

test_dataloader = PromptDataLoader(dataset=dataset["test"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=256, 
    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=True,
    truncate_method="head")
```

## 5. 构建生成式提示学习模型

```python
from openprompt import PromptForGeneration
use_cuda = True
prompt_model = PromptForGeneration(plm=plm,template=mytemplate, freeze_plm=False,tokenizer=tokenizer)
if use_cuda:
    prompt_model=  prompt_model.cuda()
```

此处原文档固定了PLM参数，效果很差，此处改为不固定。

## 6. 训练模型

定义优化器（不用定义额外的损失函数）：

```python
from transformers import AdamW
# Follow PrefixTuning（https://github.com/XiangLi1999/PrefixTuning), we also fix the language model
# only include the template's parameters in training. 修改了

no_decay = ["bias", "LayerNorm.weight"]
optimizer_grouped_parameters1 = [
    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer_grouped_parameters2 = [
{
    "params": [p for n, p in mytemplate.named_parameters() if (not any(nd in n for nd in no_decay)) and p.requires_grad],
    "weight_decay": 0.0,
},
{
    "params": [p for n, p in mytemplate.named_parameters() if any(nd in n for nd in no_decay) and p.requires_grad],
    "weight_decay": 0.0,
},
]


# Using different optimizer for prompt parameters and model parameters

optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)
optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-5, eps=1e-8)
```

设置线性warmup：

```python
from transformers.optimization import get_linear_schedule_with_warmup

tot_step  = len(train_dataloader)*5
scheduler = get_linear_schedule_with_warmup(optimizer2, 0, tot_step)
```

训练，同时优化PLM和template中的参数：

```python
import torch

# training and generation.
global_step = 0 
tot_loss = 0 
log_loss = 0
for epoch in range(5):
    prompt_model.train()
    for step, inputs in enumerate(train_dataloader):
        global_step +=1
        if use_cuda:
            inputs = inputs.cuda()
        loss = prompt_model(inputs)
        loss.backward()
        tot_loss += loss.item()
        torch.nn.utils.clip_grad_norm_(mytemplate.parameters(), 1.0)
        optimizer1.step()
        optimizer2.step()
        scheduler.step()
        optimizer1.zero_grad()
        optimizer2.zero_grad()
        if global_step % 500 ==0: 
            print("Epoch {}, global_step {} average loss: {} lr: {}".format(epoch, global_step, (tot_loss-log_loss)/500, scheduler.get_last_lr()[0]), flush=True)
            # 同时输出当前的lr
            log_loss = tot_loss
```

固定PLM的情况下loss几乎不下降，换为更新PLM参数后，训练过程如下所示：









使用内置的生成模型评价函数构建验证函数：

```python
# We provide generation a generation metric, you can also define your own. Note that it's not directly comparable to WebNLG's scripts evaluation.
from openprompt.utils.metrics import generation_metric
# Define evaluate function 
# 生成模型的参数
generation_arguments = {
    "max_length": 512,
    "max_new_tokens": None,
    "min_length": 5,
    "temperature": 1.0,
    "do_sample": False,
    "top_k": 0,
    "top_p": 0.9,
    "repetition_penalty": 1.0,
    "num_beams": 5,
    "bad_words_ids": [[628], [198]]
}

def evaluate(prompt_model, dataloader):
    generated_sentence = []
    groundtruth_sentence = []
    prompt_model.eval()

    for step, inputs in enumerate(dataloader):
        if use_cuda:
            inputs = inputs.cuda()
        _, output_sentence = prompt_model.generate(inputs, **generation_arguments)
        generated_sentence.extend(output_sentence)
        groundtruth_sentence.extend(inputs['tgt_text'])
    score = generation_metric(generated_sentence, groundtruth_sentence, "sentence_bleu")
    print("test_score", score, flush=True)
    return generated_sentence
```

