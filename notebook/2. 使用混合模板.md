# 使用混合模板（Mixed Templets）

本节内容在上一节的基础上使用混合的Templet。

## 1. 数据预处理

首先还是传统的读取数据

```python
import os
import torch
os.chdir("/home/lhc/Projects/Openprompt-learner")
from datasets import load_dataset, load_from_disk
raw_dataset = load_from_disk("datasets/super_glue")
raw_dataset['train'][0]
```

接着使用`InputExample`转换为输入数据

```python
from openprompt.data_utils import InputExample

dataset = {}
for split in ['train', 'validation', 'test']:
    dataset[split] = []
    for data in raw_dataset[split]:
        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])
        dataset[split].append(input_example)
print(dataset['train'][0])
```

## 2. 读取PLM

```python
from openprompt.plms import load_plm
plm, tokenizer, model_config, WrapperClass = load_plm("t5", "t5-base")
```

## 3. 构建混合模板

- Try more prompt!
- You can use templates other than manual template, for example the mixedtemplate is a good place to start.
- In MixedTemplate, you can use {"soft"} to denote a tunable template. More syntax and usage, please refer to `How to write a template`

此处使用的是插入四个可训练的token的模板：

```python
from openprompt.prompts import MixedTemplate

# mytemplate1 = MixedTemplate(model=plm, tokenizer=tokenizer, text='{"placeholder":"text_a"} {"soft": "Question:"} {"placeholder":"text_b"}? Is it correct? {"mask"}.')

mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text='{"placeholder":"text_a"} {"soft"} {"soft"} {"soft"} {"placeholder":"text_b"} {"soft"} {"mask"}.')
```

使用带参数的模板，查看一个包装后的例子：

```python
wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) 
print(wrapped_example)
```

```python
[[{'text': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'soft_token_ids': 1, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '', 'soft_token_ids': 2, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '', 'soft_token_ids': 3, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' the language was peeled down', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '', 'soft_token_ids': 4, 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'soft_token_ids': 0, 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'soft_token_ids': 0, 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]
```

## 4. 创建DataLoader

```python
from openprompt import PromptDataLoader

train_dataloader = PromptDataLoader(dataset=dataset["train"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")
```

## 5. 定义解析器Verbalizer

依然使用固定的词进行标签映射：

```python
from openprompt.prompts import ManualVerbalizer

# for example the verbalizer contains multiple label words in each class
myverbalizer = ManualVerbalizer(tokenizer, num_classes=2, 
                        label_words=[["yes"], ["no"], ["maybe"]])
```

## 6. 构建Prompt模型

此处使用了带参数的模板，可以尝试是否固定PLM的参数：

```python
from openprompt import PromptForClassification

use_cuda = True
prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)
if use_cuda:
    prompt_model=  prompt_model.cuda()
```

## 7. 训练模型

注意此时使用两个不同的`optimizer`分别优化PLM和Templet的参数：

```python
from transformers import AdamW, get_linear_schedule_with_warmup
loss_func = torch.nn.CrossEntropyLoss()

no_decay = ['bias', 'LayerNorm.weight']

# it's always good practice to set no decay to biase and LayerNorm parameters
optimizer_grouped_parameters1 = [
    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

# Using different optimizer for prompt parameters and model parameters
optimizer_grouped_parameters2 = [
    {'params': [p for n,p in prompt_model.template.named_parameters() if "raw_embedding" not in n]}
]

optimizer1 = AdamW(optimizer_grouped_parameters1, lr=1e-4)
optimizer2 = AdamW(optimizer_grouped_parameters2, lr=1e-3)
```

进入训练：

```python
for epoch in range(10):
    tot_loss = 0 
    for step, inputs in enumerate(train_dataloader):
        if use_cuda:
            inputs = inputs.cuda()
        logits = prompt_model(inputs)
        labels = inputs['label']
        loss = loss_func(logits, labels)
        loss.backward()
        tot_loss += loss.item()
        # 两个优化器分别step
        optimizer1.step()
        optimizer1.zero_grad()
        optimizer2.step()
        optimizer2.zero_grad()
        if step %100 ==1:
            print("Epoch {}, average loss: {}".format(epoch, tot_loss/(step+1)), flush=True)
```

训练结果如下：

```python
Epoch 0, average loss: 0.0002951298447442241
Epoch 1, average loss: 5.885917630621407e-06
Epoch 2, average loss: 2.562993131505209e-06
Epoch 3, average loss: 1.1026836688188268e-06
Epoch 4, average loss: 5.215401301938982e-07
Epoch 5, average loss: 7.301561311123805e-07
Epoch 6, average loss: 2.2351738593329173e-07
Epoch 7, average loss: 1.043081194751494e-07
Epoch 8, average loss: 7.450579531109724e-08
Epoch 9, average loss: 3.7252885221050747e-07
```

可以发现和之前相比loss小很多。

## 8. 验证

```python
validation_dataloader = PromptDataLoader(dataset=dataset["validation"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")


allpreds = []
alllabels = []
for step, inputs in enumerate(validation_dataloader):
    if use_cuda:
        inputs = inputs.cuda()
    logits = prompt_model(inputs)
    labels = inputs['label']
    alllabels.extend(labels.cpu().tolist())
    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())

acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)
```

```python
print(acc)
0.9107142857142857
```

效果有了肉眼可见提升。

## 尝试固定PLM进行训练

首先重新读取PLM：

```python
plm, tokenizer, model_config, WrapperClass = load_plm("t5", "t5-base")
```

创建一个新的Prompt模型`prompt_model2`，固定了PLM的参数：

```python
from openprompt import PromptForClassification

use_cuda = True
prompt_model2 = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=True)
if use_cuda:
    prompt_model2 = prompt_model2.cuda()
```

创建优化器，指定损失函数：

```python
optimizer_grouped_parameters3 = [
    {'params': [p for n,p in prompt_model2.template.named_parameters() if "raw_embedding" not in n]}
]

optimizer3 = AdamW(optimizer_grouped_parameters3, lr=1e-3)
loss_func = torch.nn.CrossEntropyLoss()
```

此时可训练参数就是模板中的5个token向量，参数总量$5*768$。下面开始训练：

```python
for epoch in range(10):
    tot_loss = 0
    for step, inputs in enumerate(train_dataloader):
        if use_cuda:
            inputs = inputs.cuda()
        logits = prompt_model2(inputs)
        labels = inputs['label']
        loss = loss_func(logits, labels)
        loss.backward()
        tot_loss += loss.item()
        optimizer3.step()
        optimizer3.zero_grad()
        if step %100 ==1:
            print("Epoch {}, average loss: {}".format(epoch, tot_loss/(step+1)), flush=True)
```

训练过程如下：

```python
Epoch 0, average loss: 0.8421963453292847
Epoch 1, average loss: 0.7504405081272125
Epoch 2, average loss: 1.0823779255151749
Epoch 3, average loss: 0.8208188414573669
Epoch 4, average loss: 0.7424534261226654
Epoch 5, average loss: 0.6856926381587982
Epoch 6, average loss: 0.573425829410553
Epoch 7, average loss: 0.6600835621356964
Epoch 8, average loss: 0.7140108048915863
Epoch 9, average loss: 0.6447372138500214
```

可以看到5个token的参数完全达不到模型的效果，尝试失败。