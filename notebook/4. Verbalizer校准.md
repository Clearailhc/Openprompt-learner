# Verbalizer校准

- 本节主要介绍零样本学习中的上下文校准，在本节中使用了手工template和知识嵌入的verbalizer；
- 因无论输入句子的标签如何，但有一些标签词天然地更不可能被预测到，这是由于标签词的先验分布具有很大差异。上下文校准的方法，仍然是利用标签词的上下文先验分布来校准预测的分布；
- 在本节的例子中，校准的目的是调整知识嵌入解析器中的单词数量；
- 原文为Knowledgeable Prompt-tuning（[2108.02035.pdf (arxiv.org)](https://arxiv.org/pdf/2108.02035.pdf)）.

## 1. 数据读取

本节还是使用agnews数据集进行分类任务：

```python
from openprompt.data_utils.text_classification_dataset import AgnewsProcessor

dataset = {}
dataset['train'] = AgnewsProcessor().get_train_examples("datasets/agnews")
dataset['test'] = AgnewsProcessor().get_test_examples("datasets/agnews")
```

## 2. 读取PLM

使用MLM模型`Roberta`

```python
from openprompt.plms import load_plm
plm, tokenizer, model_config, WrapperClass = load_plm("roberta", "roberta-base")
```

## 3. 定义Template

依然使用手工定义的Template，不同的是此处采用了手工定义的方法：

```python
from openprompt.prompts import ManualTemplate
mytemplate = ManualTemplate(tokenizer=tokenizer).from_file("scripts/TextClassification/agnews/manual_template.txt", choice=0)
```

手工定义的模板文件如下，共有5钟选择，此处选择第一种：

```python
 A {"mask"} news : {"placeholder": "text_a"} {"placeholder": "text_b"}
 {"placeholder": "text_a"} {"placeholder": "text_b"} This topic is about {"mask"} .
 [ Category : {"mask"} ] {"placeholder": "text_a"} {"placeholder": "text_b"}
 [ Topic : {"mask"} ] {"placeholder": "text_a"} {"placeholder": "text_b"}
 {"placeholder": "text_a"} {"placeholder": "text_b"} {"special": "<sep>"} This topic is about {"mask"} .
```

## 4. 定义verbalizer

从文档中读取知识增强的解析器：

```python
from openprompt.prompts import ManualVerbalizer, KnowledgeableVerbalizer

myverbalizer = KnowledgeableVerbalizer(tokenizer, num_classes=4).from_file("scripts/agnews/knowledgeable_verbalizer.txt")
```

具体的解析器内容中，每行包含了大量的类别相关词语。



## 5. ！！！ 数据校准！！！

首先使用`FewShotSampler`进行校准样本的提取：

```python
from openprompt.data_utils.data_sampler import FewShotSampler
support_sampler = FewShotSampler(num_examples_total=200, also_sample_dev=False)
dataset['support'] = support_sampler(dataset['train'], seed=1)
```

接着删除这些校准样本的标签：

```python
for example in dataset['support']:
    example.label = -1 # remove the labels of support set for clarification
```

构建`DataLoader`:

```python
from openprompt import PromptDataLoader

support_dataloader = PromptDataLoader(dataset=dataset["support"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, 
    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="tail")
```

构建Prompt模型：

```python
from openprompt import PromptForClassification
use_cuda = True
prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)
if use_cuda:
    prompt_model=  prompt_model.cuda()
```

先观察一下校准前各个类别Verbalizer中包含的token数目：

```python
org_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(4)]
print(org_label_words_num)
>>> [376, 350, 287, 366]
```

进行概率计算：

```python
from openprompt.utils.calibrate import calibrate
# calculate the calibration logits
cc_logits = calibrate(prompt_model, support_dataloader)

print("the calibration logits is", cc_logits)  # 词表中每个词出现的log概率
>>> the calibration logits is tensor([ 0.7094, -4.3530,  6.3426,  ..., -3.2170, -3.1547,  0.8762], device='cuda:0')
```

使用此概率对Verbalizer进行校准：

```python
# register the logits to the verbalizer so that the verbalizer will divide the calibration probability in producing label logits
# currently, only ManualVerbalizer and KnowledgeableVerbalizer support calibration.
prompt_model.verbalizer.register_calibrate_logits(cc_logits)
new_label_words_num = [len(prompt_model.verbalizer.label_words[i]) for i in range(4)]

# 查看校准前后对比
print("Original number of label words per class: {} \n After filtering, number of label words per class: {}".format(org_label_words_num, new_label_words_num))
>>> Original number of label words per class: [376, 350, 287, 366] 
 After filtering, number of label words per class: [212, 270, 218, 229]
```

可以看出删除了大量数据集中不常见的词。

## 6. 使用zero-shot进行测试

因为知识嵌入的Verbalizer中已经包含了大量的词，因此可以使用原始PLM，手工模板和校准后的解析器进行zero-shot的测试（个人局的因为Verbalizer的校准事实上是在train上进行的，有点赖皮）。

```python
# zero-shot test
import tqdm
test_dataloader = PromptDataLoader(dataset=dataset["test"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=512, decoder_max_length=3, 
    batch_size=5,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="tail")
allpreds = []
alllabels = []
pbar = tqdm(test_dataloader)
for step, inputs in enumerate(pbar):
    if use_cuda:
        inputs = inputs.cuda()
    logits = prompt_model(inputs)
    labels = inputs['label']
    alllabels.extend(labels.cpu().tolist())
    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())
acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)
print("test:", acc)  # roughly ~0.853 when using template 0
>>> test: 0.8281578947368421
```

可以看到使用了大量单词的Verbalizer，即使是不训练PLM也可以的到很好的结果。