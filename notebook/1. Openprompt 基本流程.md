# Openprompt 基本流程

> In this scripts, you will learn 
>
> 1. how to use integrate huggingface datasets utilities into openprompt to enable prompt learning in diverse datasets.
> 1. How to instantiate a template using a template language
> 1. How does the template wrap the input example into a templated one.
> 1. How do we hide the PLM tokenization details behind and provide a simple tokenization
> 1. How do construct a verbalizer using one/many label words
> 1. How to train the prompt like a traditional Pretrained Model.

在这个脚本中，我们将学习

1. 如何使用将`huggingface`中的`datasets utilities`集成到`openprompt`中，

   以便在各种数据集中实现提示学习；

2. 如何使用模板语言实例化模板；

3. 模板如何将输入示例包装到模板中；

4. 我们如何隐藏PLM令牌化的详细步骤并提供简单的令牌化；

5. 如何使用单/多标签单词构建语言解释器；

6. 如何像传统的预训练模型一样训练提示模型。

## 1.  读取数据集合

使用`datasets`包读取原始数据集

```python
# load dataset
import os
from datasets import load_dataset, load_from_disk
os.chdir("/home/lhc/Projects/Openprompt-learner")  # 更换为项目路径
# raw_dataset = load_dataset('super_glue', 'cb', cache_dir="datasets/.cache/huggingface_datasets")  # 可以尝试直接使用网络数据
raw_dataset = load_from_disk("datasets/super_glue")  # 读取本地数据
print(raw_dataset['train'][0])
```

原始数据集示例：

```python
{'premise': 'It was a complex language. Not written down but handed down. One might say it was peeled down.',
 'hypothesis': 'the language was peeled down',
 'idx': 0,
 'label': 0}
```

使用`openprompt.data_utils` 将原始数据转化为输入样例：

```python
from openprompt.data_utils import InputExample

dataset = {}
for split in ['train', 'validation', 'test']:
    dataset[split] = []
    for data in raw_dataset[split]:
        input_example = InputExample(text_a = data['premise'], text_b = data['hypothesis'], label=int(data['label']), guid=data['idx'])
        dataset[split].append(input_example)
print(dataset['train'][0])
```

数据集示例

```python
{
  "guid": 0,
  "label": 0,
  "meta": {},
  "text_a": "It was a complex language. Not written down but handed down. One might say it was peeled down.",
  "text_b": "the language was peeled down",
  "tgt_text": null
}
```

## 2. 读取PLM

> You can load the plm related things provided by openprompt simply by calling：

 使用`openprompt.plms`包读取预训练语言模型和对应的分词器、模型配置、包装器，此处以`T5-base`为例：

 ```
 from openprompt.plms import load_plm
 plm, tokenizer, model_config, WrapperClass = load_plm("t5", "t5-base")
 ```

## 3. 构建模板

> A template can be constructed from the yaml config, but it can also be constructed by directly passing arguments.

此处使用人工构建提示模板，模板如下：$\textbf{X1}\ Question:\  \textbf{X2}\ Is\ it\ correct?\  \textbf{Z}$ .

```python
template_text = '{"placeholder":"text_a"} Question: {"placeholder":"text_b"}? Is it correct? {"mask"}.'
mytemplate = ManualTemplate(tokenizer=tokenizer, text=template_text)
```

> To better understand how does the template wrap the example, we visualize one instance.

## 4. 使用包装器 （单个使用时）

使用包装器将数据中的句子套入之前定义的模板，看一个包装的示例：

```python
wrapped_example = mytemplate.wrap_one_example(dataset['train'][0]) 
print(wrapped_example)
```

```python
[[{'text': 'It was a complex language. Not written down but handed down. One might say it was peeled down.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' Question:', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': ' the language was peeled down', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '? Is it correct?', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}, {'text': '.', 'loss_ids': 0, 'shortenable_ids': 0}], {'guid': 0, 'label': 0}]
```

其中`'loss_ids'`代表是否计算loss，`'shortenable_ids'`代表是否可以截断。


> Now, the wrapped example is ready to be pass into the tokenizer, hence producing the input for language models. You can use the tokenizer to tokenize the input by yourself, but we recommend using our wrapped tokenizer, which is a wrapped tokenizer tailed for InputExample. 
>
> The wrapper has been given if you use our `load_plm` function, otherwise, you should choose the suitable wrapper based on the configuration in `openprompt.plms.__init__.py`.
>
> Note that when t5 is used for classification, we only need to pass <pad> <extra_id_0> <eos> to decoder. The loss is calcaluted at <extra_id_0>. Thus passing decoder_max_length=3 saves the space.

在使用包装分词器的时候，需要使用语言模型对应的`WrapperClass`和`tokenizer`，对应的长度和截断的方法。

```python
wrapped_t5tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method="head")
# or
from openprompt.plms import T5TokenizerWrapper
wrapped_t5tokenizer= T5TokenizerWrapper(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer,truncate_method="head")
```

看一个包装后分词的示例：

```python
tokenized_example = wrapped_t5tokenizer.tokenize_one_example(wrapped_example, teacher_forcing=False)
print(tokenized_example)
print(tokenizer.convert_ids_to_tokens(tokenized_example['input_ids']))
print(tokenizer.convert_ids_to_tokens(tokenized_example['decoder_input_ids'])
```

```python
# 模型输入的形式
{'input_ids': [94, 47, 3, 9, 1561, 1612, 5, 933, 1545, 323, 68, 14014, 323, 5, 555, 429, 497, 34, 47, 158, 400, 26, 323, 5, 11860, 10, 8, 1612, 47, 158, 400, 26, 323, 3, 58, 27, 7, 34, 2024, 58, 32099, 3, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'decoder_input_ids': [0, 32099, 0], 'loss_ids': [0, 1, 0]}

# 将ids转化为token（输入）
['▁It', '▁was', '▁', 'a', '▁complex', '▁language', '.', '▁Not', '▁written', '▁down', '▁but', '▁handed', '▁down', '.', '▁One', '▁might', '▁say', '▁it', '▁was', '▁pe', 'ele', 'd', '▁down', '.', '▁Question', ':', '▁the', '▁language', '▁was', '▁pe', 'ele', 'd', '▁down', '▁', '?', '▁I', 's', '▁it', '▁correct', '?', '<extra_id_0>', '▁', '.', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']

# # 将ids转化为token（解码输入）
['<pad>', '<extra_id_0>', '<pad>']
```

可以通过循环批量包装输入数据：

```python
model_inputs = {}
for split in ['train', 'validation', 'test']:
    model_inputs[split] = []
    for sample in dataset[split]:
        tokenized_example = wrapped_t5tokenizer.tokenize_one_example(mytemplate.wrap_one_example(sample), teacher_forcing=False)
        model_inputs[split].append(tokenized_example)
```

在普通使用的过程中，可以直接通过下一节中的`PromptDataLoader`直接构建输入。

## 5. 构建DataLoader

使用`PromptDataLoader`类直接构建输入模型的batch：

```python
from openprompt import PromptDataLoader

train_dataloader = PromptDataLoader(dataset=dataset["train"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")
# next(iter(train_dataloader))
```

## 6. 构建解释器

> In classification, you need to define your verbalizer, which is a mapping from logits on the vocabulary to the final label probability. Let's have a look at the verbalizer details:

在此处，根据目标的分类类型，构建多种词对应某一标签类别的解释器：

```python
from openprompt.prompts import ManualVerbalizer
import torch

# for example the verbalizer contains multiple label words in each class
myverbalizer = ManualVerbalizer(tokenizer, num_classes=3, 
                        label_words=[["yes", "good", "ha"], ["no", "bad"], ["maybe"]])
```

查看标签的id：

```python
print(myverbalizer.label_words_ids)
>>> Parameter containing:
tensor([[[4273],
         [ 207],
         [4244]],
        [[ 150],
         [1282],
         [   0]],
        [[2087],
         [   0],
         [   0]]])
```

构建一个虚拟的模型输出，查看解析器的效果：

```python
logits = torch.randn(2,len(tokenizer)) # creating a pseudo output from the plm, and 
print(myverbalizer.process_logits(logits)) # see what the verbalizer do
```

```python
tensor([[-2.1743, -1.1831, -4.2552],
        [-1.7657, -3.3518, -1.3635]])
```

解析器会输出的到三种标签的概率。

## 7. 构建Prompt模型

使用预定义的Pipeline方式组合plm, template, verbalizer：

```python
from openprompt import PromptForClassification

use_cuda = True
prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)
if use_cuda:
    prompt_model=  prompt_model.cuda()
```



## 8. 训练

```python
# Now the training is standard
from transformers import  AdamW, get_linear_schedule_with_warmup
loss_func = torch.nn.CrossEntropyLoss()

# it's always good practice to set no decay to biase and LayerNorm parameters
no_decay = ['bias', 'LayerNorm.weight']
optimizer_grouped_parameters = [
    {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

optimizer = AdamW(optimizer_grouped_parameters, lr=1e-4)

for epoch in range(10):
    tot_loss = 0 
    for step, inputs in enumerate(train_dataloader):
        if use_cuda:
            inputs = inputs.cuda()
        logits = prompt_model(inputs)
        labels = inputs['label']
        loss = loss_func(logits, labels)
        loss.backward()
        tot_loss += loss.item()
        optimizer.step()
        optimizer.zero_grad()
        if step %100 ==1:
            print("Epoch {}, average loss: {}".format(epoch, tot_loss/(step+1)), flush=True)
    
```

此模型固定了模板和解析器，因此训练的还是plm，训练过程如下：

```python
Epoch 0, average loss: 1.209649235010147
Epoch 1, average loss: 0.19695379491895437
Epoch 2, average loss: 0.06017631199210882
Epoch 3, average loss: 0.001950863457750529
Epoch 4, average loss: 0.0004914285818813369
Epoch 5, average loss: 0.0002415510534774512
Epoch 6, average loss: 0.00027822253832709976
Epoch 7, average loss: 0.0002275548795296345
Epoch 8, average loss: 7.719549466855824e-05
Epoch 9, average loss: 0.0001006285565381404
```

## 9. 验证

```python
# 验证dataloader
validation_dataloader = PromptDataLoader(dataset=dataset["validation"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")

allpreds = []
alllabels = []
for step, inputs in enumerate(validation_dataloader):
    if use_cuda:
        inputs = inputs.cuda()
    logits = prompt_model(inputs)
    labels = inputs['label']
    alllabels.extend(labels.cpu().tolist())
    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist()) # 预测结果

acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)
print(acc)
```

正确label和预测label如下所示：

```python
alllabels= [1, 2, 0, 1, 1, 1, 0, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 2, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 2, 1, 1, 0]

allpreds = [2, 0, 0, 1, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 1, 2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2, 1, 1, 0]
```

正确率如下：

```python
print(acc)
0.8571428571428571
```

