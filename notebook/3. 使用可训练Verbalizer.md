# 使用可训练Verbalizer

本节内容将使用可训练的Verbalizer

## 1. 数据处理

读取本地的Agnews数据集，该数据集合是一个文本分类任务：

```python
import os
os.chdir("/home/lhc/Projects/Openprompt-learner")

from openprompt.data_utils.text_classification_dataset import AgnewsProcessor

dataset = {}
dataset['train'] = AgnewsProcessor().get_train_examples("datasets/agnews")
dataset['test'] = AgnewsProcessor().get_test_examples("datasets/agnews")
```

使用`FewShotSampler`类对训练数据做少样本处理，处理之前训练集的大小为：

```python
len(dataset['train'])
120000
```

原始数据没有dev集合，因此相当于做了两次随机欠采样：

```python
from openprompt.data_utils.data_sampler import FewShotSampler
sampler  = FewShotSampler(num_examples_per_label=16, num_examples_per_label_dev=16, also_sample_dev=True)  # 每种标签取了16个样本
dataset['train'], dataset['validation'] = sampler(dataset['train'])
```

处理之后的大小：

```python
print(len(dataset['train']), len(dataset['validation']))
64 64
```

## 2. 读取PLM

```python
from openprompt.plms import load_plm

plm, tokenizer, model_config, WrapperClass = load_plm("t5", "t5-base")
```

## 3. 制定Templet

```python
from openprompt.prompts import ManualTemplate
mytemplate = ManualTemplate(tokenizer=tokenizer, text='{"placeholder":"text_a"} {"placeholder":"text_b"} In this sentence, the topic is {"mask"}.')
```

## 4. 构建DataLoader

```python
from openprompt import PromptDataLoader

train_dataloader = PromptDataLoader(dataset=dataset["train"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=True, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")
```

## 5. ！！！定义可训练verbalizer！！！

可训练的解释器，可以实现将模型输出的词表概率映射为分类标签的概率。本质上是设定k个可训练的虚拟token，然后计算模型输出和这几个token的相似度：

```python
from openprompt.prompts import SoftVerbalizer
myverbalizer = SoftVerbalizer(tokenizer, plm, num_classes=4)
# 其中label_words是可选的，作为虚拟token的初始化（此处有貌似有bug）
```

## 6. 组建模型

```python
from openprompt import PromptForClassification

use_cuda = True
prompt_model = PromptForClassification(plm=plm,template=mytemplate, verbalizer=myverbalizer, freeze_plm=False)
if use_cuda:
    prompt_model=  prompt_model.cuda()
```

## 7. 训练模型

设定优化器和损失函数：

```python
import torch
from transformers import AdamW, get_linear_schedule_with_warmup

no_decay = ['bias', 'LayerNorm.weight']

# it's always good practice to set no decay to biase and LayerNorm parameters
optimizer_grouped_parameters1 = [
    {'params': [p for n, p in prompt_model.plm.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
    {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
]

# Using different optimizer for prompt parameters and model parameters

optimizer_grouped_parameters2 = [
    {'params': prompt_model.verbalizer.group_parameters_1, "lr":3e-5},
    {'params': prompt_model.verbalizer.group_parameters_2, "lr":3e-4},
]


optimizer1 = AdamW(optimizer_grouped_parameters1, lr=3e-5)
optimizer2 = AdamW(optimizer_grouped_parameters2)
loss_func = torch.nn.CrossEntropyLoss()
```

在此处注意`verbalizer`有两组参数，它们的定义分别如下：

```python
# group_parameters_1
"""Include the parameters of head's layer but not the last layer
In soft verbalizer, note that some heads may contain modules 
other than the final projection layer. The parameters of these part should be
optimized (or freezed) together with the plm.
"""

# group_parameters_2
"""Include the last layer's parameters"""
```

在此处因为没有定义多层`verbalizer`，因此第一组参数为空，第二组参数大小为$4*768$。

接下来进入训练：

```python
for epoch in range(10):
    tot_loss = 0 
    for step, inputs in enumerate(train_dataloader):
        if use_cuda:
            inputs = inputs.cuda()
        logits = prompt_model(inputs)
        labels = inputs['label']
        loss = loss_func(logits, labels)
        loss.backward()
        tot_loss += loss.item()
        optimizer1.step()
        optimizer1.zero_grad()
        optimizer2.step()
        optimizer2.zero_grad()
    print("Epoch {}, average loss: {}".format(epoch, tot_loss/(step+1)), flush=True)
```

训练结果如下：

```python
Epoch 0, average loss: 0.33242414313708507
Epoch 1, average loss: 0.034752492641379674
Epoch 2, average loss: 0.009624250741797454
Epoch 3, average loss: 0.001570531815169878
Epoch 4, average loss: 0.0006120121122096732
Epoch 5, average loss: 0.00038510737030019446
Epoch 6, average loss: 0.000268272308410757
Epoch 7, average loss: 0.00020092648263391946
Epoch 8, average loss: 0.00015494229621022926
Epoch 9, average loss: 0.00012380963247749836
```

此处loss相比上一节中的可训练Templet，下降较慢。

## 8. 验证

首先在同样进行欠采样的dev集（64个）上验证：

```python
validation_dataloader = PromptDataLoader(dataset=dataset["validation"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")

prompt_model.eval()

allpreds = []
alllabels = []
for step, inputs in enumerate(validation_dataloader):
    if use_cuda:
        inputs = inputs.cuda()
    logits = prompt_model(inputs)
    labels = inputs['label']
    alllabels.extend(labels.cpu().tolist())
    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())

acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)
print("validation:",acc)
>>> validation: 0.875
```

接着在整体的test集（7600个）上验证：

```python
test_dataloader = PromptDataLoader(dataset=dataset["test"], template=mytemplate, tokenizer=tokenizer, 
    tokenizer_wrapper_class=WrapperClass, max_seq_length=256, decoder_max_length=3, 
    batch_size=4,shuffle=False, teacher_forcing=False, predict_eos_token=False,
    truncate_method="head")
allpreds = []
alllabels = []
for step, inputs in enumerate(test_dataloader):
    if use_cuda:
        inputs = inputs.cuda()
    logits = prompt_model(inputs)
    labels = inputs['label']
    alllabels.extend(labels.cpu().tolist())
    allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())
acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)
print("test:", acc) 
>>> test: 0.22513157894736843
```

因为训练轮数和少样本的原因，结果很垃圾。。。